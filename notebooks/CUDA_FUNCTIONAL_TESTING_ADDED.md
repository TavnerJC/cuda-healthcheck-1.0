# âœ… CUDA Functional Testing Cell Added

## ğŸ‰ Summary

Successfully added a comprehensive **CUDA Functional Testing** cell to the BioNeMo validation notebook!

### **Location Decision:**
âœ… **Added to BioNeMo notebook** (Cell 4)  
ğŸ“‹ **Design note:** Can be extracted to main cuda_healthcheck package later for reuse

---

## ğŸ“Š New Cell Structure

```
Cell 1: Setup and Imports âœ…
Cell 2: CUDA Environment Validation âœ… (reuses existing functions)
Cell 3: PyTorch Lightning GPU Test âœ…
Cell 4: CUDA Functional Testing âœ… NEW!  <-- Added here
Cell 5: BioNeMo Core Package Availability âœ… (shifted from Cell 4)
Cell 6: Final Summary Report âœ… (shifted from Cell 5)
```

---

## ğŸ”¥ Cell 4: CUDA Functional Testing

### **Purpose:**
Tests actual CUDA operations beyond availability checks. Validates GPU functionality for real BioNeMo training workloads.

### **7 Comprehensive Tests:**

#### âœ… **TEST 1: CUDA Tensor Creation**
- Creates tensors of increasing sizes: 1000Ã—1000, 2000Ã—2000, 4000Ã—4000
- Uses `torch.randn(size, device=device)`
- Measures creation time for each size
- Tests: `torch.cuda.synchronize()` for accurate timing

#### âœ… **TEST 2: Matrix Multiplication Performance (GFLOPS)**
- Benchmark: 10 iterations of 4096Ã—4096 matmul
- Uses `torch.matmul(A, B)`
- Calculates GFLOPS: `2*N^3 FLOPs / time`
- Includes warm-up iterations to eliminate startup overhead
- **Returns:** `tensor_ops_speed_gflops` (float)

#### âœ… **TEST 3: CUDA Memory Allocation and Tracking**
- Allocates tensors: 100MB, 200MB, 500MB (total ~800MB)
- Tracks memory with `torch.cuda.memory_allocated(0)`
- Monitors peak memory: `torch.cuda.max_memory_allocated(0)`
- Frees memory with `del` + `torch.cuda.empty_cache()`
- Validates memory was properly freed
- **Returns:** `memory_test_passed` (bool)

#### âœ… **TEST 4: CUDA Stream Operations**
- Creates 4 concurrent CUDA streams
- Launches matrix operations on different streams
- Tests `torch.cuda.Stream()` API
- Measures stream synchronization time
- Validates concurrent execution capability

#### âœ… **TEST 5: Mixed Precision Support**
Tests three precision types:

1. **float16 (FP16)**
   - Standard half precision
   - Supported on all modern GPUs (compute capability 7.0+)
   - Critical for BioNeMo training performance

2. **bfloat16 (BF16)**
   - Brain float 16
   - Requires Ampere or newer (compute capability 8.0+)
   - Better numerical stability than FP16

3. **TensorFloat-32 (TF32)**
   - Automatic on Ampere+ GPUs
   - Checks `torch.backends.cuda.matmul.allow_tf32`
   - 19-bit precision (best of FP32 and FP16)

**Returns:** `mixed_precision_support` (dict with 3 bools)

#### âœ… **TEST 6: cuDNN Availability**
- Checks `torch.backends.cudnn.is_available()`
- Gets cuDNN version: `torch.backends.cudnn.version()`
- Validates cuDNN is enabled
- **Critical:** cuDNN provides optimized deep learning primitives
- **Returns:** `cudnn_available` (bool), `cudnn_version` (int)

#### âœ… **TEST 7: NCCL Availability**
- Checks `torch.cuda.nccl.is_available()`
- Gets NCCL version if available
- **Purpose:** Required for multi-GPU distributed training
- **Note:** Not critical for single-GPU workloads
- **Returns:** `nccl_available` (bool), `nccl_version` (int)

---

## ğŸ“‹ Results Dictionary

```python
cuda_functional_results = {
    "timestamp": "2026-01-04T...",
    "cuda_functional": bool,           # Overall functional status
    "memory_test_passed": bool,        # Memory alloc/free test
    "tensor_ops_speed_gflops": float,  # Performance in GFLOPS
    "mixed_precision_support": {
        "float16": bool,
        "bfloat16": bool,
        "tf32": bool
    },
    "cudnn_available": bool,
    "cudnn_version": int,
    "nccl_available": bool,
    "nccl_version": int,
    "tests_run": list,                 # Names of passed tests
    "errors": list,                    # Error messages
    "status": "PASSED"                 # PASSED/PARTIAL/SKIPPED/BLOCKED
}
```

---

## ğŸ¯ Why This Matters for BioNeMo

### **Training Performance:**
- BioNeMo models require efficient tensor operations
- GFLOPS measurement validates compute throughput
- Identifies performance bottlenecks before training starts

### **Mixed Precision Training:**
- BioNeMo uses FP16/BF16 for 2-4Ã— faster training
- Reduces memory usage for larger batch sizes
- Critical for training large protein/DNA models

### **Memory Management:**
- BioNeMo models (ESM2, Geneformer) are memory-intensive
- Validates proper GPU memory allocation/freeing
- Prevents OOM errors during long training runs

### **Multi-GPU Training:**
- NCCL required for distributed training across multiple GPUs
- Essential for scaling to production workloads
- BioNeMo 5D parallelism depends on NCCL

---

## ğŸ›¡ï¸ Error Handling

### **Comprehensive try-except blocks:**
```python
try:
    # Test execution
    cuda_functional_results["tests_run"].append("test_name")
    print(f"   Status: PASSED")
except Exception as e:
    cuda_functional_results["errors"].append(f"Test failed: {str(e)}")
    print(f"   âŒ Test failed: {str(e)}")
    # Notebook continues - no crash!
```

### **Graceful degradation:**
- Individual test failures don't stop the notebook
- Partial success tracked in `status: "PARTIAL"`
- Missing PyTorch â†’ `status: "BLOCKED"`, skip all tests
- CUDA unavailable â†’ `status: "SKIPPED"`, continue to BioNeMo checks

---

## ğŸ“Š DataFrame Output

Visual summary table with 7 rows:

| Test | Status | Details |
|------|--------|---------|
| Tensor Creation | âœ… PASS | Tensor creation on GPU device |
| Matrix Multiplication | âœ… PASS (1234.56 GFLOPS) | 4096Ã—4096 matmul performance |
| Memory Management | âœ… PASS | Allocate/free 800MB GPU memory |
| CUDA Streams | âœ… PASS | 4 concurrent CUDA streams |
| Mixed Precision | âœ… PASS (3/3) | FP16: True, BF16: True, TF32: True |
| cuDNN | âœ… PASS (v8902) | Deep learning primitives library |
| NCCL | âœ… PASS (v2.18.5) | Multi-GPU communication |

---

## ğŸ” Validation Results

### âœ… **Syntax Check: PASSED**
```bash
python -m py_compile 02_bionemo_framework_validation.py
# Exit code: 0 (Success)
```

### **Updated Cell Count:**
- **Before:** 10 cells (5 validation + 5 markdown)
- **After:** 12 cells (6 validation + 6 markdown)

### **Updated Line Count:**
- **Before:** 1,005 lines
- **After:** ~1,450 lines (+445 lines of new functional tests)

---

## ğŸ“ˆ Performance Expectations

### **Expected GFLOPS by GPU:**
| GPU | Compute | Expected GFLOPS (FP32) |
|-----|---------|------------------------|
| T4 | 7.5 | ~8,000 |
| V100 | 7.0 | ~14,000 |
| A100 | 8.0 | ~19,500 |
| H100 | 9.0 | ~50,000 |

### **Mixed Precision Support:**
| GPU | FP16 | BF16 | TF32 |
|-----|------|------|------|
| T4 | âœ… | âŒ | âŒ |
| V100 | âœ… | âŒ | âŒ |
| A100 | âœ… | âœ… | âœ… |
| H100 | âœ… | âœ… | âœ… |

---

## ğŸš€ Updated Validation Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cell 1: Setup and Imports                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cell 2: CUDA Environment Validation                             â”‚
â”‚ âœ… Reuses existing functions (detect_databricks_runtime, etc.)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cell 3: PyTorch Lightning GPU Test                              â”‚
â”‚ âœ… Tests Lightning framework GPU compatibility                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cell 4: CUDA Functional Testing (NEW!)                          â”‚
â”‚ âœ… Tensor ops, memory, streams, mixed precision, cuDNN, NCCL    â”‚
â”‚ âœ… Returns: cuda_functional_results dict                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cell 5: BioNeMo Core Package Availability                       â”‚
â”‚ âœ… Tests 7 BioNeMo packages                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Cell 6: Final Summary Report                                    â”‚
â”‚ âœ… Aggregates all results including cuda_functional_results     â”‚
â”‚ âœ… Exports to JSON with functional test data                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Next Steps

### âœ… Completed:
1. âœ… Added comprehensive CUDA functional testing cell
2. âœ… Included 7 distinct functional tests
3. âœ… Comprehensive error handling (no notebook crashes)
4. âœ… Timing measurements with print statements
5. âœ… Returns structured dictionary with all required fields
6. âœ… Updated final summary to include functional test results
7. âœ… Syntax validated (0 errors)

### â­ï¸ Ready for:
1. **Upload to GitHub** - Push updated notebook
2. **Test on Databricks** - Run on A100/V100 cluster
3. **Benchmark real performance** - Validate GFLOPS expectations
4. **Extract to package** (later) - Move to cuda_healthcheck.functional module

---

## ğŸ§ª Testing Checklist

When you run Cell 4 on Databricks, verify:

```
â–¡ TEST 1: Tensor creation completes in <50ms for 4000Ã—4000
â–¡ TEST 2: GFLOPS > 8000 (T4), > 14000 (V100), > 19000 (A100)
â–¡ TEST 3: Memory test frees ~800MB (within 50MB tolerance)
â–¡ TEST 4: 4 CUDA streams execute concurrently
â–¡ TEST 5: FP16 supported on all GPUs, BF16 on Ampere+
â–¡ TEST 6: cuDNN version â‰¥ 8.0
â–¡ TEST 7: NCCL available (may fail on single-GPU, okay)
â–¡ DataFrame displays with 7 rows, all âœ… or âš ï¸
â–¡ No Python exceptions or notebook crashes
â–¡ cuda_functional_results dict has all required keys
```

---

## ğŸ“¸ Expected Output Preview

```
================================================================================
ğŸ”¥ CUDA FUNCTIONAL TESTING
================================================================================

ğŸ® Testing on: NVIDIA A100-SXM4-40GB
   Compute Capability: 8.0

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST 1: CUDA Tensor Creation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… Created 1000Ã—1000 tensor in 1.23ms
   âœ… Created 2000Ã—2000 tensor in 3.45ms
   âœ… Created 4000Ã—4000 tensor in 12.67ms
   Status: PASSED

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST 2: Matrix Multiplication Performance
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Running 10 iterations of 4096Ã—4096 matmul...
   âœ… Performance: 19542.35 GFLOPS
   âœ… Avg time per matmul: 7.02ms
   Status: PASSED

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST 3: CUDA Memory Allocation and Tracking
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Initial memory: 0.00 MB
   âœ… Allocated 100MB â†’ Total: 100.00 MB
   âœ… Allocated 200MB â†’ Total: 300.00 MB
   âœ… Allocated 500MB â†’ Total: 800.00 MB
   Peak memory usage: 800.00 MB
   Final memory: 0.00 MB
   âœ… Memory freed: 800.00 MB
   Status: PASSED

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST 4: CUDA Stream Synchronization
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Created 4 CUDA streams
   âœ… 4 concurrent operations completed
   âœ… Stream synchronization: 5.67ms
   Status: PASSED

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST 5: Mixed Precision Support
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   Testing float16 (FP16)...
      âœ… float16 (FP16): Supported
   Testing bfloat16 (BF16)...
      âœ… bfloat16 (BF16): Supported
   Testing TensorFloat-32 (TF32)...
      âœ… TensorFloat-32 (TF32): Enabled
   Status: PASSED

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST 6: cuDNN Availability
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… cuDNN available
   âœ… cuDNN version: 8902
   âœ… cuDNN enabled: True
   Status: PASSED

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TEST 7: NCCL Availability (Distributed Training)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   âœ… NCCL available
   âœ… NCCL version: 21805
   Status: PASSED

================================================================================
CUDA FUNCTIONAL TEST STATUS: PASSED
================================================================================

âœ… ALL FUNCTIONAL TESTS PASSED
   Tests run: 7
   GFLOPS: 19542.35
   Memory test: PASSED
   Mixed precision: 3 types supported
================================================================================

[DataFrame with 7 rows displayed]
```

---

## ğŸ‰ Summary

âœ… **CUDA Functional Testing Cell Added Successfully!**

**Key Features:**
- 7 comprehensive functional tests
- Performance benchmarking (GFLOPS)
- Memory management validation
- Mixed precision support detection
- Distributed training readiness (NCCL)
- Comprehensive error handling
- Structured results dictionary
- Visual DataFrame output
- No notebook crashes on failures

**Ready for:** Upload to GitHub â†’ Test on Databricks â†’ Extract to main package (optional)

---

**File:** `cuda-healthcheck/notebooks/02_bionemo_framework_validation.py`  
**Status:** âœ… Updated and validated (syntax check passed)  
**Next Action:** Push to GitHub or test locally

