# Databricks Deployment Guide

This guide explains how to deploy and run the CUDA Healthcheck Tool on Databricks GPU clusters.

## ğŸ¯ Quick Start

### 1. Import the Notebook

**Option A: Direct URL Import**
1. In Databricks, go to **Workspace** â†’ **Import**
2. Select **URL**
3. Paste: `https://raw.githubusercontent.com/TavnerJC/cuda-healthcheck-1.0/main/notebooks/databricks_healthcheck.py`
4. Click **Import**

**Option B: Clone the Repository**
1. In Databricks, go to **Repos** â†’ **Add Repo**
2. Git URL: `https://github.com/TavnerJC/cuda-healthcheck-1.0`
3. Navigate to `notebooks/databricks_healthcheck.py`

### 2. Create a GPU Cluster

**Minimum Requirements:**
- **Runtime:** Databricks Runtime 13.3 LTS ML or higher
- **Instance Type:** GPU-enabled (g5.xlarge, g5.4xlarge, p3.2xlarge, etc.)
- **Python:** 3.10+

**Example Cluster Configuration:**
```
Cluster Mode: Standard
Databricks Runtime: 13.3 LTS ML (includes Apache Spark 3.4.1, GPU, Scala 2.12)
Worker Type: g5.4xlarge (1 GPU, 16 vCPUs, 64 GB RAM)
Workers: 1-4 (autoscaling)
Driver Type: i3.xlarge (no GPU needed)
```

### 3. Run the Notebook

1. Attach the notebook to your GPU cluster
2. Run all cells sequentially
3. Review the output

---

## ğŸ“Š What Gets Detected

### Cell 3: GPU Detection
- Physical GPU count and models
- CUDA driver version
- GPU memory
- Compute capability
- Number of Spark executors

### Cell 4: Breaking Changes
- PyTorch compatibility issues
- TensorFlow compatibility issues
- RAPIDS/cuDF compatibility issues
- CUDA version transition risks
- Compatibility scores (0-100)

---

## ğŸ—ï¸ Architecture

### Driver vs Worker Nodes

Databricks clusters have a **driver-worker architecture**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Driver Node    â”‚  â† Notebooks run here (usually no GPU)
â”‚  (i3.xlarge)    â”‚  â† Package installed here via %pip
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         â”‚         â”‚         â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â” â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚Worker 1â”‚ â”‚Worker 2â”‚ â”‚Worker 3â”‚ â”‚Worker 4â”‚ â† GPUs are here!
â”‚(g5.4xl)â”‚ â”‚(g5.4xl)â”‚ â”‚(g5.4xl)â”‚ â”‚(g5.4xl)â”‚ â† 16 executors per worker
â”‚1x A10G â”‚ â”‚1x A10G â”‚ â”‚1x A10G â”‚ â”‚1x A10G â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Points:**
- `%pip install` only installs on the **driver**
- GPUs are on the **workers**
- We use Spark to run detection on workers
- Results are collected back to the driver

---

## ğŸ”§ Advanced: Full Distributed Healthcheck

For complete healthcheck functionality on workers (not just GPU detection), install the package **cluster-wide**:

### Method 1: Cluster Libraries (Recommended)

1. Go to your cluster configuration
2. Click **Libraries** tab
3. Click **Install New** â†’ **PyPI**
4. Enter: `git+https://github.com/TavnerJC/cuda-healthcheck-1.0.git`
5. Click **Install**
6. **Restart the cluster**

Now you can run the full `DatabricksHealthchecker` on workers!

### Method 2: Init Script

Create an init script to install on cluster startup:

```bash
#!/bin/bash
pip install git+https://github.com/TavnerJC/cuda-healthcheck-1.0.git
```

Upload to DBFS and configure in cluster settings.

---

## ğŸ“ Example Output

### Successful Detection

```
================================================================================
ğŸ–¥ï¸  DRIVER NODE
================================================================================
Driver: No GPU detected (expected for driver node)

================================================================================
ğŸ® WORKER NODES - GPU DETECTION
================================================================================
ğŸ“Š Cluster Configuration:
   Spark Executors: 16
   Unique Worker Nodes: 1

ğŸ“ Worker Node 1: ip-10-0-1-234.ec2.internal
   Physical GPUs: 1
      GPU 0: NVIDIA A10G
         Driver: 535.161.07
         Memory: 23028 MiB
         Compute Capability: 8.6

================================================================================
âœ… ACTUAL PHYSICAL GPUs in cluster: 1
   (Detected 16 times - once per Spark executor)
================================================================================
```

### Compatibility Analysis

```
================================================================================
ğŸ” CUDA BREAKING CHANGES ANALYSIS
================================================================================

ğŸ“¦ PyTorch Breaking Changes:
   âœ… Found 2 PyTorch breaking changes

ğŸ“¦ TensorFlow Breaking Changes:
   âœ… Found 2 TensorFlow breaking changes

ğŸ”„ CUDA Version Transition Analysis:
   CUDA 11.8 â†’ 12.0: 2 breaking changes
   CUDA 12.0 â†’ 13.0: 6 breaking changes

================================================================================
ğŸ’¯ COMPATIBILITY SCORING
================================================================================

ğŸ“Š CUDA 12.0 Compatibility:
   Score: 100/100
   Critical: 0 | Warnings: 0
   Status: GOOD: Environment is highly compatible.

ğŸ“Š CUDA 13.0 Compatibility:
   Score: 40/100
   Critical: 2 | Warnings: 0
   Status: CRITICAL: Breaking changes detected. Test before upgrading!

================================================================================
```

---

## âš ï¸ Common Issues

### Issue 1: "No GPU detected on driver"

**Expected!** The driver node typically doesn't have a GPU. GPUs are on worker nodes.

### Issue 2: "Package import fails on workers"

**Solution:** Install package cluster-wide (see Advanced section above).

### Issue 3: "16 GPUs detected but only 1 physical GPU"

**Expected!** Each Spark executor reports the GPU. The code deduplicates by UUID to show actual physical GPUs.

### Issue 4: "Cell hangs with py4j messages"

**Cause:** Trying to import package on workers when it's only installed on driver.  
**Solution:** Use the provided notebook which avoids package imports on workers for basic detection.

---

## ğŸ¯ Use Cases

### 1. Pre-Deployment Validation
Run before deploying ML models to verify CUDA compatibility.

### 2. Cluster Configuration Audit
Validate that your cluster has the expected GPU configuration.

### 3. Framework Upgrade Planning
Check compatibility scores before upgrading PyTorch, TensorFlow, or CUDA.

### 4. Breaking Changes Detection
Identify critical issues before they cause production failures.

### 5. Multi-Cluster Comparison
Run on different clusters to compare configurations.

---

## ğŸ“š Additional Resources

- [Main README](../README.md) - Full documentation
- [API Reference](../docs/API_REFERENCE.md) - Detailed API docs
- [Local Testing](../TESTING_AND_NOTEBOOKS_SUMMARY.md) - Run tests locally
- [CI/CD](../docs/CICD.md) - GitHub Actions workflows

---

## ğŸ’¡ Tips

1. **Run regularly:** Add to your cluster startup routine
2. **Before upgrades:** Always check compatibility scores
3. **Save results:** Export to Delta table for historical tracking
4. **Team sharing:** Share the notebook with your ML team
5. **Custom checks:** Extend the notebook for your specific needs

---

## ğŸ†˜ Support

- **GitHub Issues:** [Report bugs or request features](https://github.com/TavnerJC/cuda-healthcheck-1.0/issues)
- **Documentation:** Check the [main README](../README.md)
- **Examples:** See the [notebooks folder](../notebooks/)

---

**Happy GPU Healthchecking!** ğŸ‰

